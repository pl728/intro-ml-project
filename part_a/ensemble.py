from project.utils import *
import random
import torch

from sklearn.ensemble import BaggingClassifier
import project.part_a.knn as knn
import project.part_a.item_response as irt
import project.part_a.neural_network as nn
from sklearn.impute import KNNImputer


def irt_predict(data, theta, beta):
    pred = []
    for i, q in enumerate(data["question_id"]):
        u = data["user_id"][i]
        x = (theta[0][u] - beta[0][q]).sum()
        p_a = irt.sigmoid(x)
        pred.append(p_a)
    return pred


def main():
    # ------------------ITEM RESPONSE ENSEMBLE (3 itr ensemble) -------------------
    alpha = 0.05
    iterations = 30

    # load train, val, test
    train_data = load_train_csv("../data")
    val_data = load_valid_csv("../data")
    test_data = load_public_test_csv("../data")

    # get number of data points N
    N = len(train_data['user_id'])

    # sample n training examples
    bootstrap_sample_size = N
    d1 = random.choices([i for i in range(N)], k=bootstrap_sample_size)
    d2 = random.choices([i for i in range(N)], k=bootstrap_sample_size)
    d3 = random.choices([i for i in range(N)], k=bootstrap_sample_size)

    train_data1, train_data2, train_data3 = {'user_id': [], 'question_id': [], 'is_correct': []}, \
                                            {'user_id': [], 'question_id': [], 'is_correct': []}, \
                                            {'user_id': [], 'question_id': [], 'is_correct': []}

    # construct 3 new datasets from bootstrap sample indices
    for i in range(N):
        train_data1['user_id'].append(train_data['user_id'][d1[i]])
        train_data1['question_id'].append(train_data['question_id'][d2[i]])
        train_data1['is_correct'].append(train_data['is_correct'][d3[i]])

        train_data2['user_id'].append(train_data['user_id'][d2[i]])
        train_data2['question_id'].append(train_data['question_id'][d2[i]])
        train_data2['is_correct'].append(train_data['is_correct'][d2[i]])

        train_data3['user_id'].append(train_data['user_id'][d3[i]])
        train_data3['question_id'].append(train_data['question_id'][d3[i]])
        train_data3['is_correct'].append(train_data['is_correct'][d3[i]])

    # train
    train_theta1, train_beta1, _, _ = irt.irt(train_data1, train_data1, alpha, iterations)
    train_theta2, train_beta2, _, _ = irt.irt(train_data2, train_data2, alpha, iterations)
    train_theta3, train_beta3, _, _ = irt.irt(train_data3, train_data3, alpha, iterations)

    prediction1 = irt_predict(test_data, train_theta1, train_beta1)
    prediction2 = irt_predict(test_data, train_theta2, train_beta2)
    prediction3 = irt_predict(test_data, train_theta3, train_beta3)
    all_predictions = np.array([prediction1, prediction2, prediction3])

    avg_prediction = np.mean(all_predictions, axis=0)
    avg_prediction = np.round(avg_prediction)
    average_probability_accuracy = np.sum((test_data["is_correct"] == np.array(avg_prediction))) / len(test_data["is_correct"])

    majority_vote = np.round(all_predictions)
    majority_vote = np.round(np.sum(majority_vote, axis=0) / 3)
    majority_vote_accuracy = np.sum((test_data["is_correct"] == np.array(majority_vote))) / len(test_data["is_correct"])

    print("average probability accuracy: " + str(average_probability_accuracy))
    print('majority vote accuracy: ' + str(majority_vote_accuracy))

    separator = 'separator'

    # ----------------------- kNN ensemble ---------------------------------
    # NOTE: bootstrap samples are generated by selecting random rows (users) from the sparse matrix
    # sparse_matrix = load_train_sparse("../data").toarray()
    # val_data = load_valid_csv("../data")
    # test_data = load_public_test_csv("../data")
    #
    # # value of k
    # k_star = 21
    #
    # # bootstrap sampling
    # bootstrap_sample_size = len(sparse_matrix)
    # d1 = sparse_matrix[np.random.randint(sparse_matrix.shape[0], size=bootstrap_sample_size), :]
    # d2 = sparse_matrix[np.random.randint(sparse_matrix.shape[0], size=bootstrap_sample_size), :]
    # d3 = sparse_matrix[np.random.randint(sparse_matrix.shape[0], size=bootstrap_sample_size), :]
    #
    # nbrs1 = KNNImputer(n_neighbors=k_star)
    # nbrs2 = KNNImputer(n_neighbors=k_star)
    # nbrs3 = KNNImputer(n_neighbors=k_star)
    #
    # # We use NaN-Euclidean distance measure.
    # # model 1, 3 are imputed by user, model 2 is imputed by question
    # mat1 = nbrs1.fit_transform(d1)
    # mat2 = nbrs2.fit_transform(d2.T)
    # mat3 = nbrs3.fit_transform(d3)
    #
    # total_predictions = 0
    # total_accurate = 0
    # for i in range(len(test_data["is_correct"])):
    #     cur_user_id = test_data["user_id"][i]
    #     cur_question_id = test_data["question_id"][i]
    #
    #     # calculate votes
    #     bagged_prediction = np.sum(np.round(np.array([mat1[cur_user_id, cur_question_id],
    #                                                   mat2.T[cur_user_id, cur_question_id],
    #                                                   mat3[cur_user_id, cur_question_id]]))) / 3
    #
    #     if bagged_prediction >= 0.5 and test_data["is_correct"][i]:
    #         total_accurate += 1
    #     if bagged_prediction < 0.5 and not test_data["is_correct"][i]:
    #         total_accurate += 1
    #
    #     total_predictions += 1
    #
    # avg_accuracy = total_accurate / float(total_predictions)
    #
    # print("Test Accuracy (3 x kNN ensemble): {}".format(avg_accuracy))
    # ----------------------------------------------------------------------------------------

if __name__ == "__main__":
    main()
